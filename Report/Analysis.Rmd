---
title: "Analysis"
author: "Elodie Kwan and Katia Voltz"
date: '2022-04-26'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE, message=FALSE}
source(here::here("scripts/setup.R"))
```

### Import libraries and data

```{r}
German_credit_cleaned <- read.csv("./../Data_DA/GermanCredit_cleaned.csv", header = TRUE)

German_credit_cleaned$DURATION <- as.numeric(German_credit_cleaned$DURATION)
German_credit_cleaned$AMOUNT <- as.numeric(German_credit_cleaned$AMOUNT)
German_credit_cleaned$INSTALL_RATE <- as.numeric(German_credit_cleaned$INSTALL_RATE)
German_credit_cleaned$AGE <- as.numeric(German_credit_cleaned$AGE)
German_credit_cleaned$NUM_CREDITS <- as.numeric(German_credit_cleaned$NUM_CREDITS)
German_credit_cleaned$NUM_DEPENDENTS <- as.numeric(German_credit_cleaned$NUM_DEPENDENTS)

for (i in 1:ncol(German_credit_cleaned)){
  if (class(German_credit_cleaned[,i])=="integer"){
    German_credit_cleaned[,i] <- factor(German_credit_cleaned[,i])
    }
}

str(German_credit_cleaned)
```

### Traning set and Test set 
In order to fit some model, we are going to split our dataset into 2 sets. The **training set** and the **test set**. We do not forget to take the first variable **OBS.** out as it represents the index number for each observation. 

```{r}
set.seed(1234)
# indexes of the training set observations 
index.tr <- sample(x = 1:nrow(German_credit_cleaned), size= 3/4*nrow(German_credit_cleaned), replace=FALSE)
# training set 
German_credit.tr <- German_credit_cleaned[index.tr,-1]
# test set 
German_credit.te <- German_credit_cleaned[-index.tr,-1]
```


### Fitting a model : Classification Tree (Decision Tree)

Let's try a classification tree 
```{r}
german.ct <- rpart(RESPONSE ~ ., method = "class", data = German_credit.tr )
summary(german.ct)
```

Then we plot the classification tree. 

```{r}
par(pty = "s", mar = c(1, 1, 1, 1))
plot(german.ct, cex = 1)
text(german.ct, cex = 0.6)

rpart.plot(german.ct)
```

#### Pruning the tree 

```{r}
plotcp(german.ct)
```

From the graph, we can identify that, according to the 1-SE rule, the tree with 4 nodes is equivalent to the tree with 10 nodes. This 4-nodes tree should be preferred.

The value of cp = 0.029 has been chosen arbitrarily between 0.016 and 0.052.

```{r}
german.ct.prune <- prune(german.ct, cp=0.029)
rpart.plot(german.ct.prune)
```

We decide to also look at what would an automatically pruned using 1-SE rule would give use. 
```{r}
set.seed(1234)
rpart.plot(autoprune(RESPONSE ~ ., method = "class", data = German_credit.tr))
```
Using the pruned tree we made by hand, we can compute the prediction and build a confusion matrix.

```{r}
german.ct.prediction <- predict(german.ct.prune, newdata=German_credit.te, type="class")

# Confusion Matrix
table(Pred=german.ct.prediction, Obs=German_credit.te$RESPONSE)
```

For the prediction of "bad" applicant, our model is not good. We need to balance our data.

### Fitting another model : Logistic Regression 
```{r}
# Logistic regression to see the significant variables (not working)
mod1 <- glm(RESPONSE~., data = German_credit_cleaned[, -1], family= binomial)
summary(mod1)
```



## Next Analysis 

- Balance the data using either method. 
- Then, using **caret** and either CV or Bootstrap, put several models in competition. 
- Select the best one according to your choice of score. 
- Finally, use the test set to see if the best model does not overfit the training set.

By doing this, we will have achieved a complete supervised learning task from A to Z.


## 1/ Balancing the dataset

In this part, we apply the balancing data technique in order to improve the prediction of “Good Credit” and "Bad Credit". The table below reveals the unbalance problem.

```{r}
## Statistics on the training set
table(German_credit.tr$RESPONSE)
```

Indeed, we observe that the 'Good Credit' (1) response appears 527 times in the training set and 'Bad Credit' (0) 223, two times less.
Since there are many more 'Good Credit' than 'Bad Credit', any model favors the prediction of the'Good Credit'. It results a good accuracy but the specificity is low, as well as the balanced accuracy.


### Sub-sampling
Balancing using sub-sampling consists of taking all the cases in the smallest class (here “Bad Credit”) and extract at random the same amount of cases in the largest category (here “Good”).

```{r}
n.Bad_Credit <- min(table(German_credit.tr$RESPONSE))

German_credit.tr.Bad_Credit <- filter(German_credit.tr, RESPONSE == 0) ## the "0" cases
German_credit.tr.Good_Credit <- filter(German_credit.tr, RESPONSE == 1) ## The "1" cases

index.no <- sample(size=n.Bad_Credit, x=1:nrow(German_credit.tr.Good_Credit), replace=FALSE) 
  ## sub-sample 840 instances from the "No"

German_Credit.tr.subs <- data.frame(rbind(German_credit.tr.Bad_Credit, German_credit.tr.Good_Credit[index.no,])) 
  ## Bind all the "Yes" and the sub-sampled "No"

table(German_Credit.tr.subs$RESPONSE) ## The cases are balanced
```

The dataset is now balanced.


### Cross-validation with caret
The 10-CV can be easily obtained from **caret**. 

First, set up the splitting data method using the **trainControl** function.

```{r}
German_Credit.trctrl <- trainControl(method = "cv", number=10)
```

```{r}
German.Credit.cv <- train(RESPONSE ~., data = German_credit.tr,
                method = "glmStepAIC", 
                family="binomial",
                trControl=trctrl, trace=0)
German.Credit.cv
```


```{r}
German_Credit.pred <- predict(German.Credit.cv, newdata = German_credit.te)

confusionMatrix(data=German_Credit.pred, reference = German_credit.te$RESPONSE)
```



