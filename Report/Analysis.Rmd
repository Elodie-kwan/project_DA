---
title: "Analysis"
author: "Elodie Kwan and Katia Voltz"
date: '2022-04-26'
output:
  pdf_document: default
  html_document: default
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
source("../scripts/setup.R")
```

## Methodology

In this section we will talk about the methodology that has been used and the different models analysis that has been conducted. 

```{r include=FALSE}
German_credit_cleaned <- read.csv("./../Data_DA/GermanCredit_cleaned.csv", header = TRUE)

German_credit_cleaned$DURATION <- as.numeric(German_credit_cleaned$DURATION)
German_credit_cleaned$AMOUNT <- as.numeric(German_credit_cleaned$AMOUNT)
German_credit_cleaned$INSTALL_RATE <- as.numeric(German_credit_cleaned$INSTALL_RATE)
German_credit_cleaned$AGE <- as.numeric(German_credit_cleaned$AGE)
German_credit_cleaned$NUM_CREDITS <- as.numeric(German_credit_cleaned$NUM_CREDITS)
German_credit_cleaned$NUM_DEPENDENTS <- as.numeric(German_credit_cleaned$NUM_DEPENDENTS)

for (i in 1:ncol(German_credit_cleaned)){
  if (class(German_credit_cleaned[,i])=="integer"){
    German_credit_cleaned[,i] <- factor(German_credit_cleaned[,i])
    }
}

str(German_credit_cleaned)
```

### Traning set and Test set 

First of all we started by splitting our dataset into 2 sets: **training set** (`German_credit.tr`) and **test set** (`German_credit.te`). We do not forget to take the first variable **OBS.** out as it represents the index number for each observation. These two sets will allow us to train some models on the **training set** and then test the accuracy of the model fit on the **test set**. 

```{r include=FALSE}
set.seed(1234)
# indexes of the training set observations 
index.tr <- sample(x = 1:nrow(German_credit_cleaned), 
                   size= 3/4*nrow(German_credit_cleaned), replace=FALSE)
# training set 
German_credit.tr <- German_credit_cleaned[index.tr,-1]
# test set 
German_credit.te <- German_credit_cleaned[-index.tr,-1]
```

### Balancing the dataset

Then, we applied the balancing data technique in order to improve the predictions of **Good Credit** and **Bad Credit**, since we have more observations on the **Good Credit**.


```{r include=FALSE}
## Statistics on the training set
table(German_credit.tr$RESPONSE)
```

Indeed, we observe that the "Good Credit" (1) response appears **527** times in the training set and "Bad Credit" (0) **223**, two times less.
Since there are many more "Good Credit" than "Bad Credit", any model favors the prediction of the "Good Credit". It results a good accuracy but the specificity is low, as well as the balanced accuracy.


#### Sub-sampling

Balancing using sub-sampling consists of taking all the cases in the smallest class (here “Bad Credit”) and extract at random the same amount of cases in the largest category (here “Good”).

```{r include = FALSE}
n.Bad_Credit <- min(table(German_credit.tr$RESPONSE)) ## 32

## the "0" cases
German_credit.tr.Bad_Credit <- filter(German_credit.tr, RESPONSE == 0) 

## The "1" cases
German_credit.tr.Good_Credit <- filter(German_credit.tr, RESPONSE == 1)

## sub-sample 32 instances from the "Good Credit" by drawing indexes
index.no <- sample(size=n.Bad_Credit, x=1:nrow(German_credit.tr.Good_Credit), replace=FALSE) 

## Bind all the "Bad" and the sub-sampled "Good"
German_Credit.tr.subs <- data.frame(rbind(German_credit.tr.Bad_Credit,
                                          German_credit.tr.Good_Credit[index.no,])) 

## The cases are balanced
table(German_Credit.tr.subs$RESPONSE) 
```

The **training set** is now balanced, we have 223 observations for both "Good Credit" (1) and "Bad Credit" (0). The new balanced training set is called `German_Credit.tr.subs`. 


### Models Fitting

Once we had our training set and test set, we could fit some models and compare them with together to choose the best model. 

#### 1. Classification Tree (Decision Tree)
<br>
We first started with a decision tree and more specifically we chose the classification tree as we want to classify the applicants. The model was build on our previously balanced training set `German_Credit.tr.subs`. We used the R function `rpart`. 

```{r include = FALSE}
german.ct <- rpart(RESPONSE ~ ., method = "class", data = German_Credit.tr.subs )
summary(german.ct)
```

We obtained the following large tree. 

```{r echo = FALSE, fig.align = 'center'}
rpart.plot(german.ct)
```

We could see that among the 31 explanatory variables, this model uses 6 of them: **CHK_ACCT**, **AMOUNT**, **HISTORY**, **DURATION**, **GUARANTOR** and **EMPLOYMENT**. 

```{r echo = FALSE}
german.bigct.prediction <- predict(german.ct, 
                                   newdata=German_credit.te,
                                   type="class")

cm_bigct <- confusionMatrix(data=german.bigct.prediction ,
                            reference = German_credit.te$RESPONSE)

pander(cm_bigct$table, 
       caption = "Confusion Matrix of the Big classification tree",
       row.names = c("Bad credit risk", "Good credit risk"),
       col.names = c("Bad credit risk", "Good credit risk"))

cm_bigct$byClass

```

<br>
We first have an insight on how well it predict the test set (`German_credit.te`
). We recall that 0 means a "Bad Credit" risk and 1 means a "Good Credit" risk. It seems that this model has difficulty to predict the "Bad Credit" risk applicants. Indeed from the table have 70 observations that were misclassified as being "Bad credit" while it was in fact a "Good credit". 


As the tree is quite big and we want to know if we can prune it. To do so, we decided to use the `printcp` and `plotcp` commands and choose the best **cp** (complexity parameter) value to prune our tree. 

##### Pruning the tree 

```{r echo = FALSE, fig.align = 'center'}
printcp(german.ct)

plotcp(german.ct)
```

From the list of **cp** (complexity parameter), we would choose the line that has the lowest cross validation error. This can be seen on the column **xerror**. So the best cp would be 0.022422 with one split.  

From the graph, we can identify that, according to the 1-SE rule, the tree with 2 and 3 are equivalent. The tree with 3 nodes should be preferred. It appears below the dotted-line. 

The value of cp can be chosen arbitrarily between 0.018 and 0.095. So we decided to go with the suggested cp of 0.022 from the summary. 

With these value, we obtain a very small tree. 

```{r echo = FALSE, fig.align = 'center'}
german.ct.prune <- prune(german.ct, cp=0.022)
rpart.plot(german.ct.prune)
```

This pruned decision tree with a cp of 0.022 uses the variables **CHK_ACCT** and **HISTORY**. 

Using this pruned tree, we can computed the prediction and build a confusion matrix to see the performance of the model.

```{r echo = FALSE}
german.ct.prediction <- predict(german.ct.prune, newdata=German_credit.te, type="class")

# Confusion Matrix
cm_ctprune <- confusionMatrix(data=german.ct.prediction, 
                              reference = German_credit.te$RESPONSE)

pander(cm_ctprune$table, 
       caption = "Confusion Matrix of the Pruned classification tree",
       row.names = c("Bad credit risk", "Good credit risk"),
       col.names = c("Bad credit risk", "Good credit risk"))

cm_ctprune$byClass
```

Here again, this model has difficulty to predict the "Bad credit" cases. The model also decreased in its precision values.


We also decided to look at what would an automatically pruned using 1-SE rule would give us and whether or not it is better than the pruned tree we made by looking at the cp.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
set.seed(1234)
german.ct.autoprune <- autoprune(RESPONSE ~ ., method = "class",
                                 data = German_Credit.tr.subs)
rpart.plot(german.ct.autoprune )
```


Here, only the variable **CHK_ACCT** is used. As we prune the tree more information are lost. 


```{r echo = FALSE}
german.ct.autoprune.prediction <- predict(german.ct.autoprune,
                                          newdata=German_credit.te,
                                          type="class")

# Confusion Matrix
cm_ctautoprune <- confusionMatrix(data=german.ct.autoprune.prediction,
                                  reference = German_credit.te$RESPONSE)

pander(cm_ctautoprune$table, 
       caption = "Confusion Matrix of the Autoruned classification tree",
       row.names = c("Bad credit risk", "Good credit risk"),
       col.names = c("Bad credit risk", "Good credit risk"))

cm_ctautoprune$byClass
```

Again, it seems that in general the classification trees we perfomed have difficulty to predict the "Bad credit" cases. 

##### Variable importance of the classification tree 
<br>
Then we analysed the variable importance of one of the models. We decided to compute the variable importance of the **pruned classification tree** `german.ct.prune` by applying the function `varImp` on the model. 

```{r include=FALSE}
varImp(german.ct.prune)
```


It is summarized below on the plot. 

```{r fig.align='center', echo=FALSE}
vip(german.ct.prune)
```

From this plot, we see that the variables that influences the most are **CHK_ACCT**, **HISTORY**, **SAV_ACCT**, **DURATION**, **AMOUNT** and **REAL_ESTATE**. They are not exactly the same as the one we saw above.

The variable **CHK_ACCT** and **HISTORY** were already noticed though. 



#### 2. Logistic Regression  
<br> 
The next model we performed is a logistic regression. 
```{r echo=FALSE}

logreg <- glm(RESPONSE~., data = German_Credit.tr.subs, family= binomial)
summary(logreg)
```

We see that a lot of variables are not statistically significant for the model so we can think of a model reduction.

Before doing a reduction of the model, we fitted the model and predicted on the test set. 

```{r echo=FALSE}
german.logreg.pred_prob <- predict(logreg, newdata = German_credit.te,
                                    type="response")

german.logreg.pred <- ifelse(german.logreg.pred_prob >= 0.5, 1, 0)

# Confusion Matrix
cm_logreg <- confusionMatrix(data=as.factor(german.logreg.pred),
                             reference = German_credit.te$RESPONSE)

pander(cm_logreg$table, 
       caption = "Confusion Matrix of the Logistic Regression",
       row.names = c("Bad credit risk", "Good credit risk"),
       col.names = c("Bad credit risk", "Good credit risk"))

cm_logreg$byClass
```

From the confusion matrix, we see again the the model has difficulty to predict the "Bad credit" although the wronged classified amount of observation is lower when it comes to the "Bad credit" one.

##### Variable selection and interpretation with step method (AIC criteria)
<br>
In order to reduce the logistic regression we used a stepwise variable selection. This has been done with the command `step`. 

```{r include = FALSE}
mod.logreg.sel <- step(logreg) # store the final model into mod.logreg.sel
```

The final reduced model is as follow. 

```{r echo=FALSE}
summary(mod.logreg.sel)
```

The variables that have been removed are : **FURNITURE**, **RADIO.TV**, **EDUCATION**, **RETRAINING**, **MALE_DIV**, **MALE_SINGLE**, **MALE_MAR_or_WID**, **CO.APPLICANT**, **REAL_ESTATE**, **OWN_RES**, **NUM_CREDITS**, **JOB** and **NUM_DEPENDENTS**

In the end, we get the most significant model: 

$$ 
RESPONSE = -0.6339113 + 0.5970566 * CHK_{ACCT1} + 1.1123874 * CHK_{ACCT2} + \\
2.4109175 * CHK_{ACCT3} - 0.6393459 * HISTORY1 + 0.3153810 * HISTORY2 \\
+ 0.0245812 * HISTORY3 + 1.0638624 * HISTORY4 - 0.7159178 * NEW_{CAR1} \\
+ 1.3489217 * USED_{CAR1}  -0.8489518 * RETRAINING1 - 0.0002631 * AMOUNT \\
+ 0.5675624 * SAV_{ACCT1} - 0.0693577 * SAV_{ACCT2} + 0.5603771 * SAV_{ACCT3} \\
+ 1.3592948 * * SAV_{ACCT4} + 0.5542570 * EMPLOYMENT1 + 1.2338686 * EMPLOYMENT2 \\
+ 1.7999683 * EMPLOYMENT3 + 1.5521376 * EMPLOYMENT4 - 0.3278020 * INSTALL_{RATE} \\
+ 1.6927223 * GUARANTOR1 - 1.1117822 * PRESENT_{RESIDENT2} \\
- 0.3408387 * PRESENT_{RESIDENT3} - 0.7613632 * PRESENT_{RESIDENT4} \\
- 1.0532655 * PROP_{UNKN_NONE1} + 0.0181856 * AGE - 0.6281982 * OTHER_{INSTALL1} \\
- 0.8736712 * RENT1 + 0.5251823 * TELEPHONE + 1.2896516 * FOREIGN 
$$

<br> 

$$ 
p = (e^{RESPONSE})/(1 + e^{RESPONSE})
$$

It means that : 

+ The predicted probability of being a good applicant for **CHCK_ACCT3** is higher than for **CHK_ACCT0** (and also higher than for **CHK_ACCT1** and **CHK_ACCT2**).
+ The predicted probability of being a good applicant for **HISTORY1** is lower than for **HISTORY0**.
+ The predicted probability of being a good applicant for **HISTORY4** is higher than for **HISTORY0** (and also higher than for **HISTORY2** and **HISTORY3**).
+ The predicted probability of being a good applicant for **NEW_CAR1** is lower than for **NEW_CAR0**.
+ The predicted probability of being a good applicant for **USED_CAR1** is higher than for **USED_CAR0**.
+ The predicted probability of being a good applicant for **RETRAINING1** is lower than for **RETRAINING0**.
+ **AMOUNT** is negatively associated with **RESPONSE**.
+ The predicted probability of being a good applicant for **SAV_ACCT4** is higher than for **SAV_ACCT0** (and also higher than for **SAV_ACCT1** and **SAV_ACCT3**).
+ The predicted probability of being a good applicant for **SAV_ACCT2** lower than for **SAV_ACCT0**.
+ The predicted probability of being a good applicant for **EMPLOYMENT3** is higher than for **Employment0** (and also higher than for **EMPLOYMENT1**, **EMPLOYMENT2** and **EMPLOYMENT4**).
+ **INSTALL_RATE** is negatively associated with **RESPONSE**.
+ The predicted probability of being a good applicant for **GUARANTOR1 ** is higher than for **GUARANTOR0**.
+ The predicted probability of being a good applicant for **PRESENT_RESIDENT2** is lower than for **PRESENT_RESIDENT0** (and also lower than **PRESENT_RESIDENT3** and **PRESENT_RESIDENT4**).
+ The predicted probability of being a good applicant for **PROP_UNKN_NONE1** is lower than for **PROP_UNKN_NONE0**.
+ **AGE** is positively associated with **RESPONSE**.
+ The predicted probability of being a good applicant for **OTHER_INSTALL1** is lower than for **OTHER_INSTALL0**.
+ The predicted probability of being a good applicant for **RENT1** is lower than for **RENT0**.
+ The predicted probability of being a good applicant for **TELEPHONE1** is higher than for **TELEPHONE0**.
+ The predicted probability of being a good applicant for **FOREIGN1** is higher than for **FOREIGN0**.


```{r echo = FALSE}
german.logregsel.pred_prob <- predict(mod.logreg.sel, newdata = German_credit.te,
                                    type="response")

german.logregsel.pred <- ifelse(german.logregsel.pred_prob  >= 0.5, 1, 0)

# Confusion Matrix
cm_logregsel <- confusionMatrix(data=as.factor(german.logregsel.pred),
                                reference = German_credit.te$RESPONSE)

pander(cm_logregsel$table, 
       caption = "Confusion Matrix of the AIC reduced Logistic regression",
       row.names = c("Bad credit risk", "Good credit risk"),
       col.names = c("Bad credit risk", "Good credit risk"))

cm_logregsel$byClass
```

From this point we might think that the problem for the difficulty to predict the "Bad Credit" cases is not due to the models but rather the data itself. We might still want to perfom other models to make sure of our intuition.

##### Variable importance for logistic regression
<br> 

```{r include=FALSE}
x_train <- select(German_Credit.tr.subs, -RESPONSE)
y_train <- pull(German_Credit.tr.subs, RESPONSE)

explainer_logreg <- explain(model = mod.logreg.sel, 
                               data = x_train, 
                               y = as.numeric(y_train), 
                               label = "AIC reduced Logistic Regression")


importance_logreg  <- calculate_importance(explainer_logreg)
```

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
plot(importance_logreg)
```


```{r echo=FALSE, fig.align='center'}
vip(mod.logreg.sel)
```

Listed above are the most important variables for the logarithmic regression we reduced. Here again, the **CHK_ACCT** variable differentiate itself from the others in term of importance in the model prediction. 

#### 3.a K-Nearest Neighbor (K=2)
<br> 
To perform a k-nearest neighbor method, we do not need to balance the data so we will use the unbalanced training set.

We first try to model it using a 2-NN (with Euclidean distance).
Note that the model is fitting on the training set and the predictions are computed on the test set.

```{r echo=FALSE}
set.seed(123)
# applying Knn model with k = 2 on the training set
German_credit.knn2.tr <- knn3(data=German_credit.tr, RESPONSE~., k=2)  

German_credit.knn2.tr.pred <- predict(German_credit.knn2.tr, 
                                     newdata = German_credit.te, type="class")
```


```{r echo=FALSE}
# Confusion Matrix
cm_knn2 <- confusionMatrix(data=German_credit.knn2.tr.pred,
                           reference = German_credit.te$RESPONSE)

pander(cm_knn2$table, 
       caption = "Confusion Matrix of the 2-Nearest neighbor",
       row.names = c("Bad credit risk", "Good credit risk"),
       col.names = c("Bad credit risk", "Good credit risk"))

cm_knn2$byClass
```


The table is read as follow : 

+ We predicted 21 Bad credits and there were indeed 21 observed Bad credits. But the prediction misjudges 45 good credits by predicting them as bad credits.
+ We predicted 128 Good credits as it was in fact a Good credits but 56 where predicted as Good while it was in fact Bad.

So again, this model also have difficulties to predict the "Bad Credit" cases.

The prediction is not perfect. We need to try to improve the prediction by changing K at that point. Therefore, we use K=3.

#### 3.b K-Nearest Neighbor (K=3)

```{r echo=FALSE}
set.seed(123)
# applying Knn model with k = 3 on the training set
German_credit.knn3.tr <- knn3(data=German_credit.tr, RESPONSE~., k=3)  

German_credit.knn3.tr.pred <- predict(German_credit.knn3.tr, 
                                      newdata = German_credit.te, type="class")
```


```{r echo=FALSE}
# Confusion Matrix
cm_knn3 <- confusionMatrix(data=German_credit.knn3.tr.pred,
                           reference = German_credit.te$RESPONSE)

pander(cm_bigct$table, 
       caption = "Confusion Matrix of the 3-Nearest neighbor",
       row.names = c("Bad credit risk", "Good credit risk"),
       col.names = c("Bad credit risk", "Good credit risk"))

cm_knn3$byClass
```

The table is read as follow : 

+ We predicted 58 Bad credits and they were indeed observed Bad credits. But the prediction misjudges 70 good credits by predicting them as being bad credits.
+ We predicted 103 Good credits as it was in fact a Good credits but 19 where predicted as Good while it was in fact Bad.

Again it seems not to have improved anything, the **F-measure** even seems to have decreased a little bit.

#### 4. Linear Support Vector Machine
<br>
The next model is the linear Support Vector Machine. 

```{r echo=FALSE}
German_credit.svm <- svm(RESPONSE ~ ., data=German_Credit.tr.subs, kernel="linear")
German_credit.svm
German_credit.svm.pred <- predict(German_credit.svm, newdata = German_credit.te)
```

```{r echo=FALSE}
cm_linearsvm <- confusionMatrix(data=German_credit.svm.pred, 
                                reference = German_credit.te$RESPONSE )

pander(cm_linearsvm$table, 
       caption = "Confusion Matrix of the Linear support vector machine",
       row.names = c("Bad credit risk", "Good credit risk"),
       col.names = c("Bad credit risk", "Good credit risk"))

cm_linearsvm$byClass
```

This model seems to have improved a little bit our ability to predict the "Bad Credit" cases. As half of the "Bad Credit" predicted by the model are correct. 



##### Tunning the hyperparameters of Linear SVM
<br> 
We want to select the good hyperparameters for our linear SVM.

```{r echo=FALSE}
German_Credit.trctrl <- trainControl(method = "cv", number=10)
```


```{r echo=FALSE}
set.seed(143)
svm_Linear <- train(RESPONSE ~., data = German_Credit.tr.subs, method = "svmLinear",
                    trControl=German_Credit.trctrl)
svm_Linear
```

<br> 
We see that we have a good accuracy (0.72). 
<br> 

```{r cache = TRUE, echo = FALSE, fig.align='center'}
grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))

set.seed(143)
svm_Linear_Grid <- train(RESPONSE ~., data = German_Credit.tr.subs,
                         method = "svmLinear",
                         trControl=German_Credit.trctrl,
                         tuneGrid = grid)
svm_Linear_Grid

plot(svm_Linear_Grid)
```

```{r echo=FALSE}
German_credit.lsvm.tuned <- svm(RESPONSE ~ .,data = German_Credit.tr.subs,
                              kernel = "linear", 
                              cost = svm_Linear_Grid$bestTune$C)

German_credit.lsvm.tuned.pred <- predict(German_credit.lsvm.tuned, 
                                       newdata = German_credit.te)

cm_linearsvm_tuned <- confusionMatrix(data=German_credit.lsvm.tuned.pred,
                                      reference = German_credit.te$RESPONSE)

pander(cm_linearsvm_tuned$table, 
       caption = "Confusion Matrix of the Tuned linear support vector machine",
       row.names = c("Bad credit risk", "Good credit risk"),
       col.names = c("Bad credit risk", "Good credit risk"))

cm_linearsvm_tuned$byClass
```

Again, half of the "Bad Credit" predicted observations are indeed "Bad credit". 

The Linear support vector models are not too bad in the sense that they are better that the models we have seen so far.


#### 5. Radial Basis Support Vector Machine
We try now with a radial basis kernel (the default).

```{r echo=FALSE}
German_credit.rbsvm <- svm(RESPONSE ~ ., data=German_Credit.tr.subs, kernel="radial")
German_credit.rbsvm

German_credit.rbsvm.pred <- predict(German_credit.rbsvm, 
                                    newdata = German_credit.te)

cm_rbsvm <- confusionMatrix(data=German_credit.rbsvm.pred, 
                            reference = German_credit.te$RESPONSE )

pander(cm_rbsvm$table, 
       caption = "Confusion Matrix of the Radial base support vector machine",
       row.names = c("Bad credit risk", "Good credit risk"),
       col.names = c("Bad credit risk", "Good credit risk"))

cm_rbsvm$byClass
```

This model performance is very close to the linear support vector models. We need to infer more by looking at the tunned version.

##### Tunning the hyperparameters of Radial basis SVM

```{r cache=TRUE, echo=FALSE}
grid_radial <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),
                           C = c(1, 10, 100, 500, 1000))
```

```{r cache = TRUE, echo=FALSE}
set.seed(143)

svm_Radial_Grid <- train(RESPONSE ~., data = German_Credit.tr.subs, 
                         method = "svmRadial",
                         trControl=German_Credit.trctrl,
                         tuneGrid = grid_radial)

svm_Radial_Grid
```


```{r echo=FALSE, fig.align='center'}
plot(svm_Radial_Grid)

svm_Radial_Grid$bestTune
```


```{r echo=FALSE}
German_credit.rbsvm.tuned <- svm(RESPONSE ~ .,data = German_Credit.tr.subs,
                                 kernel = "radial", 
                                 gamma = svm_Radial_Grid$bestTune$sigma,
                                 cost = svm_Radial_Grid$bestTune$C)

German_credit.rbsvm.tuned.pred <- predict(German_credit.rbsvm.tuned,
                                          newdata = German_credit.te)

cm_rbsvm_tuned <- confusionMatrix(data=German_credit.rbsvm.tuned.pred,
                                  reference = German_credit.te$RESPONSE)

pander(cm_rbsvm_tuned$table, 
       caption = "Confusion Matrix of the Tuned radial base support vector machine",
       row.names = c("Bad credit risk", "Good credit risk"),
       col.names = c("Bad credit risk", "Good credit risk"))

cm_rbsvm_tuned$byClass
```

The model seems to be well balanced and it has good scores. The F-measure is quite high as well.


#### 6. Neural Network - Simple hyperparameter tuning

To select the good parameters, we build a search grid and fit the model with each possible value in the grid. This is brute force and time consuming. The best model is selected among all the possible choices.

```{r cache = TRUE, message = FALSE, warning = FALSE, results = 'hide', echo=FALSE}
set.seed(1)
fitControl <- trainControl(method = "cv", 
                           number = 10)

nnetGrid <-  expand.grid(size = seq(from = 1, to = 6, by = 1),
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))

nnetFit <- train(RESPONSE ~ ., 
                 data = German_Credit.tr.subs,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)

```

```{r echo=FALSE, fig.align='center'}
plot(nnetFit)
```
The best Neural Networks parameters would be to choose 3 hidden layers, with a decay of 0.4. 

```{r include=FALSE}
set.seed(345)
nn3 <- nnet(RESPONSE ~ ., data=German_Credit.tr.subs, size=3, decay = 0.4)

nn3_pred <- predict(nn3, newdata = German_credit.te, type="class")
```


```{r echo=FALSE}
cm_nn3 <- confusionMatrix(data= as.factor(nn3_pred),
                          reference = German_credit.te$RESPONSE)

pander(cm_nn3$table, 
       caption = "Confusion Matrix of the Hyperparameter tuned neural network 3 nodes",
       row.names = c("Bad credit risk", "Good credit risk"),
       col.names = c("Bad credit risk", "Good credit risk"))

cm_nn3$byClass
```

We see that this Neural Network competes quite close to the support vector machines ones. Almost half of the "Bad credit" cases that it has predicted are correct. 

#### 7. Gradient Boosting 

The Gradient Boosting model accepts only numerical values so we have some transformation to do on our data in order to use it. 

```{r echo = FALSE}
Gradient_boost.y_train <- as.integer(German_Credit.tr.subs$RESPONSE)-1
Gradient_boost.y_test <- as.integer(German_credit.te$RESPONSE)-1

Gradient_boost.X_train <- sparse.model.matrix(RESPONSE ~ .-1, 
                                              data = German_Credit.tr.subs )
Gradient_boost.X_test <- sparse.model.matrix(RESPONSE ~ .-1,
                                             data = German_credit.te )

```


```{r echo = FALSE}
set.seed(123)
xgb_train <- xgb.DMatrix(data = Gradient_boost.X_train,
                         label = Gradient_boost.y_train)

xgb_test <- xgb.DMatrix(data = Gradient_boost.X_test, 
                        label = Gradient_boost.y_test)

xgb_params <- list(
  booster = "gbtree",
  eta = 0.01,
  max_depth = 8,
  gamma = 4,
  subsample = 0.75,
  colsample_bytree = 1,
  objective = "multi:softmax",
  eval_metric = "mlogloss",
  num_class = 2
  )

xgb_model <- xgb.train(
  params = xgb_params,
  data = xgb_train,
  nrounds = 5000,
  verbose = 1
  )

xgb_model
```


```{r echo=FALSE}
xgb_preds <- predict(xgb_model, Gradient_boost.X_test, reshape = TRUE)
```

Here we have an accuracy of 68.4%. It is good but there is room for improvement. 

```{r echo=FALSE}
cm_xgb <- confusionMatrix(data=as.factor(xgb_preds),
                          reference = as.factor(Gradient_boost.y_test))

pander(cm_xgb$table, 
       caption = "Confusion Matrix of the Gadient boosting",
       row.names = c("Bad credit risk", "Good credit risk"),
       col.names = c("Bad credit risk", "Good credit risk"))
cm_xgb$byClass
```

------------------------------------------

## Review of statistics

Once all the models were modelized we compared them according to their scores and metrics. Below we summarized all their accuracy into one table. 


```{r echo=FALSE}
models_stat <- cbind(append(cm_bigct$overall, cm_bigct$byClass[11]),
                     append(cm_ctprune$overall, cm_ctprune$byClass[11]),
                     append(cm_ctautoprune$overall, cm_ctautoprune$byClass[11]),
                     append(cm_logreg$overall, cm_logreg$byClass[11]),
                     append(cm_logregsel$overall, cm_logregsel$byClass[11]),
                     # cm_knn2$overall,
                     # cm_knn3$overall,
                     append(cm_linearsvm$overall, cm_linearsvm$byClass[11]),
                     append(cm_linearsvm_tuned$overall, cm_linearsvm_tuned$byClass[11]),
                     append(cm_rbsvm$overall, cm_rbsvm$byClass[11]),
                     append(cm_rbsvm_tuned$overall, cm_rbsvm_tuned$byClass[11]),
                     append(cm_nn3$overall, cm_nn3$byClass[11]),
                     append(cm_xgb$overall, cm_xgb$byClass[11])
                     )

colnames(models_stat) <- c("Big classification tree", 
                          "Pruned classification tree", 
                          "Autoprune classification tree", 
                          "Logistic regression", 
                          "AIC reduced Logistic regression",
                          # "2-Nearest neighbor",
                          # "3-Nearest neighbor", 
                          "Linear support vector machine",
                          "Tuned linear support vector machine",
                          "Radial base support vector machine",
                          "Tuned radial base support vector machine",
                          "Hyperparameter tuned neural network 3 nodes",
                          "Gradient Boosting"
                          )

rownames(models_stat) <- c("Accuracy", "Kappa", "Accuracy lower bound", 
                           "Accuracy upper bound", "Accuracy null", 
                           "Accuracy P-value", "Mcnemar P-value", "Balanced Accuracy")

pander(models_stat, digits = 4, split.table = 190,
       caption = "Scores of the models") 


```

According to these two first tables, the best model would be the "Radial base linear support vector machine" as it has the highest accuracy level of 0.7 and the best kappa value of 0.3628. 

The accuracy means that out of total number of observations, the model predicted correctly 70% of them. The Cohen’s Kappa Coefficient means that there is 36% of agreement, indicating that the raters agree in their classification for 36% of the cases. 

We also remember that the model was almost better than the other models when it came to predict the "Bad credit" cases. Although the model remains quite poor at predicting them since only half of the predicted ones are correct. 

Another table is done to compare the KNN because they were not performed on the balanced dataset. 


```{r echo=FALSE}

KNNmodels_stat <- cbind(cm_knn2$overall,
                        cm_knn3$overall
                        )

colnames(KNNmodels_stat) <- c("2-Nearest neighbor",
                             "3-Nearest neighbor"
                             )

pander(KNNmodels_stat, digits = 4, caption = "Scores of the KNN models")
```


Overall, we see that the worst model is the 'Autoprune classification tree'. This is understandable because we pruned the model so much that we lost many observations on the way. 



