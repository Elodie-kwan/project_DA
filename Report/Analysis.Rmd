---
title: "Analysis"
author: "Elodie Kwan and Katia Voltz"
date: '2022-04-26'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE, message=FALSE}
source(here::here("scripts/setup.R"))
```

### Import libraries and data

```{r}
German_credit_cleaned <- read.csv("./../Data_DA/GermanCredit_cleaned.csv", header = TRUE)

German_credit_cleaned$DURATION <- as.numeric(German_credit_cleaned$DURATION)
German_credit_cleaned$AMOUNT <- as.numeric(German_credit_cleaned$AMOUNT)
German_credit_cleaned$INSTALL_RATE <- as.numeric(German_credit_cleaned$INSTALL_RATE)
German_credit_cleaned$AGE <- as.numeric(German_credit_cleaned$AGE)
German_credit_cleaned$NUM_CREDITS <- as.numeric(German_credit_cleaned$NUM_CREDITS)
German_credit_cleaned$NUM_DEPENDENTS <- as.numeric(German_credit_cleaned$NUM_DEPENDENTS)

for (i in 1:ncol(German_credit_cleaned)){
  if (class(German_credit_cleaned[,i])=="integer"){
    German_credit_cleaned[,i] <- factor(German_credit_cleaned[,i])
    }
}

str(German_credit_cleaned)
```

### Traning set and Test set 
In order to fit some model, we are going to split our dataset into 2 sets. The **training set** and the **test set**. We do not forget to take the first variable **OBS.** out as it represents the index number for each observation. 

```{r}
set.seed(1234)
# indexes of the training set observations 
index.tr <- sample(x = 1:nrow(German_credit_cleaned), 
                   size= 3/4*nrow(German_credit_cleaned), replace=FALSE)
# training set 
German_credit.tr <- German_credit_cleaned[index.tr,-1]
# test set 
German_credit.te <- German_credit_cleaned[-index.tr,-1]
```

### Balancing the dataset

In this part, we apply the balancing data technique in order to improve the predictions of “Good Credit” and "Bad Credit", since we have more observations on the "Bad Credit".

The table below reveals the unbalanced problem.

```{r}
## Statistics on the training set
table(German_credit.tr$RESPONSE)
```

Indeed, we observe that the "Good Credit" (1) response appears **527** times in the training set and "Bad Credit" (0) **223**, two times less.
Since there are many more "Good Credit" than "Bad Credit", any model favors the prediction of the "Good Credit". It results a good accuracy but the specificity is low, as well as the balanced accuracy.


#### Sub-sampling
Balancing using sub-sampling consists of taking all the cases in the smallest class (here “Bad Credit”) and extract at random the same amount of cases in the largest category (here “Good”).

```{r}
n.Bad_Credit <- min(table(German_credit.tr$RESPONSE)) ## 32

## the "0" cases
German_credit.tr.Bad_Credit <- filter(German_credit.tr, RESPONSE == 0) 

## The "1" cases
German_credit.tr.Good_Credit <- filter(German_credit.tr, RESPONSE == 1)

## sub-sample 32 instances from the "Good Credit" by drawing indexes
index.no <- sample(size=n.Bad_Credit, x=1:nrow(German_credit.tr.Good_Credit), replace=FALSE) 

## Bind all the "Bad" and the sub-sampled "Good"
German_Credit.tr.subs <- data.frame(rbind(German_credit.tr.Bad_Credit,
                                          German_credit.tr.Good_Credit[index.no,])) 

## The cases are balanced
table(German_Credit.tr.subs$RESPONSE) 
```

The dataset is now balanced.


### Fitting a model : Classification Tree (Decision Tree)

Let's try a classification tree 
```{r}
german.ct <- rpart(RESPONSE ~ ., method = "class", data = German_Credit.tr.subs )
summary(german.ct)
```

Then we plot the classification tree. 

```{r}
par(pty = "s", mar = c(1, 1, 1, 1))
plot(german.ct, cex = 1)
text(german.ct, cex = 0.6)

rpart.plot(german.ct)
```

The tree is quite big and we want to know if we can prune it. 

#### Pruning the tree 

```{r}
printcp(german.ct)

plotcp(german.ct)
```

From the list of **cp** (complexity parameter), we would choose the line that has the lowest cross valiadation error. This can be seen on the column **xerror**. So the best cp would be 0.022422 with one split.  

From the graph, we can identify that, according to the 1-SE rule, the tree with 2 and 3 are equivalent. The tree with 3 nodes should be preferred. Tt appears below the dotted-line. 

The value of cp can be chosen arbitrarily between 0.018 and 0.095. So we decided to go with the suggested cp of 0.022 from the summary. 

```{r}
german.ct.prune <- prune(german.ct, cp=0.022)
rpart.plot(german.ct.prune)
```

Using the pruned tree we made by hand, we can compute the prediction and build a confusion matrix.

```{r}
german.ct.prediction <- predict(german.ct.prune, newdata=German_credit.te, type="class")

# Confusion Matrix
confusionMatrix(data=german.ct.prediction, reference = German_credit.te$RESPONSE)
```

The accuracy of the classification tree model is not really good (0.564), however we see that the balanced accuracy (0.6345) is a little bit better but still not enough. 


We decide to also look at what would an automatically pruned using 1-SE rule would give use and compare the model. 
```{r}
set.seed(1234)
german.ct.autoprune <- autoprune(RESPONSE ~ ., method = "class",
                                 data = German_Credit.tr.subs)
rpart.plot(german.ct.autoprune )
```

```{r}
german.ct.autoprune.prediction <- predict(german.ct.autoprune,
                                          newdata=German_credit.te,
                                          type="class")

# Confusion Matrix
confusionMatrix(data=german.ct.autoprune.prediction,
                reference = German_credit.te$RESPONSE)
```

The accuracy of the classification tree model is not really good (0.584) althought it is a little bit better than the one we fitted by hand. We also see that the balanced accuracy (0.6418) is a little bit better than its accuracy and the balanced accuracy of the other model, but still not enough.


#### Variable importance of the classification tree 

```{r}
varImp(german.ct.prune)
vip(german.ct.prune)
```


### Fitting another model : Logistic Regression 
```{r}
# Logistic regression to see the significant variables (not working)
logreg <- glm(RESPONSE~., data = German_Credit.tr.subs, family= binomial)
summary(logreg)
```


We can predict using the logistic regression. 

```{r}
german.logrer.pred_prob <- predict(logreg, newdata = German_credit.te,
                                    type="response")

german.logrer.pred <- ifelse(german.logrer.pred_prob >= 0.5, 1, 0)

# Confusion Matrix
confusionMatrix(data=as.factor(german.logrer.pred),
                reference = German_credit.te$RESPONSE)
```

From the confusion matrix summary, this model is better than the classification tree as the accuracy is higher (0.704) and that the balanced accuracy seems to be almost good as well ( 0.6852)

!!! To check with normal data, it seems it is pretty good too !!!

#### Variable selection and interpretation with step method (AIC criteria)

The stepwise variable selection is performed. 

```{r}
step(logreg) # see the result

mod.logreg.sel <- step(logreg) # store the final model into mod.lm.sel
summary(mod.logreg.sel)
```
removed --> FURNITURE, RADIO.TV, EDUCATION, RETRAINING, MALE_DIV, MALE_SINGLE, MALE_MAR_or_WID, CO.APPLICANT, REAL_ESTATE, OWN_RES, NUM_CREDITS, JOB, NUM_DEPENDENTS
In the end, we get the most significant model



#### Variable importance for logistic regression

```{r}
x_train <- select(German_Credit.tr.subs, -RESPONSE)
y_train <- pull(German_Credit.tr.subs, RESPONSE)

explainer_logreg <- explain(model = logreg, 
                               data = x_train, 
                               y = as.numeric(y_train), 
                               label = "Logistic Regression")


importance_logreg  <- calculate_importance(explainer_logreg)
plot(importance_logreg)
```

```{r}
vip(logreg)
```


### Fitting another model : KNN
To perform a k-nearest neighbor method, we do not need to balance the data so we will use the unbalanced training set.
Now, we make the prediction using a 2-NN (with Euclidean distance).

```{r}
German_credit.num.response <- German_credit_cleaned %>% 
  select('RESPONSE','DURATION', 'AMOUNT', 
         'INSTALL_RATE', 'AGE', 'NUM_CREDITS','NUM_DEPENDENTS')
```

```{r}
## build the K-NN model with k = 2 
German_credit.knn <- knn3(data=German_credit.num.response, RESPONSE~., k=2) 

German_credit.knn.pred <- predict(German_credit.knn, 
                                  newdata = German_credit.num.response, type="class")

German_credit.knn.pred
```

Now we can use the 2-NN to predict the test set using the training set. 
Note that the model is fitting on the training set and the predictions are computed on the test set.

```{r}
# applying Knn model with k = 2 on the training set
German_credit.knn.tr <- knn3(data=German_credit.tr, RESPONSE~., k=2)  

German_credit.knn.tr.pred <- predict(German_credit.knn.tr, 
                                     newdata = German_credit.te, type="class")
```

To compare the predictions above and the true species (the one in the test set), we can build a table. It is called a **confusion matrix** (again, this will be detailed later on).

```{r}
table(Pred=German_credit.knn.tr.pred, Observed=German_credit.te$RESPONSE)
```

The table is read as follow : 

+ We predicted 13 Bad credits and there were indeed 13 observed Bad credits. But the prediction misjudges 45 good credits by predicting bad credits.
+ We predicted 143 Good credits as it was in fact a Good credits 

```{r}
# Confusion Matrix
confusionMatrix(data=German_credit.knn.tr.pred,
                reference = German_credit.te$RESPONSE)
```

The accuracy (0.6) and the unbalanced accuracy (0.5020) are both a bit too low. 

The prediction is not perfect. We need to try to improve the prediction by changing K at that point. Therefore, we use K=3.

```{r}
# applying Knn model with k = 3 on the training set
German_credit.knn3.tr <- knn3(data=German_credit.tr, RESPONSE~., k=3)  

German_credit.knn3.tr.pred <- predict(German_credit.knn3.tr, 
                                      newdata = German_credit.te, type="class")

table(Pred=German_credit.knn3.tr.pred, Observed=German_credit.te$RESPONSE)
```


```{r}
# Confusion Matrix
confusionMatrix(data=German_credit.knn3.tr.pred,
                reference = German_credit.te$RESPONSE)
```

Both the accuracy (0.636) and the balanced data (0.5100) improved a little bit with k = 3 compared to k = 2. 
The accuracies might still be a bit low.


### Linear Support Vector Machine


...

```{r}
German_credit.svm <- svm(RESPONSE ~ ., data=German_Credit.tr.subs, kernel="linear")
German_credit.svm
German_credit.svm.pred <- predict(German_credit.svm, newdata = German_credit.te)

# confusion matrix 
table(Pred=German_credit.svm.pred, obs=German_credit.te$RESPONSE)
```

```{r}
confusionMatrix(data=German_credit.svm.pred, reference = German_credit.te$RESPONSE )
```

The accuracy (0.692) and the balanced accuracy (0.6802) are lower than 0.75 which means that it might not be good enough. 

### Radial basis Support Vector Machine
We try now with a radial basis kernel (the default).

```{r}
German_credit.rb <- svm(RESPONSE ~ ., data=German_Credit.tr.subs, kernel="radial")
German_credit.rb

German_credit.pred <- predict(German_credit.rb, newdata = German_credit.te)
confusionMatrix(data=German_credit.pred, reference = German_credit.te$RESPONSE )
```
The accuracy is better, 70%, compared to 69% with the linear method.



### Tunning the hyperparameters of Linear SVM

We want to select the good hyperparameters for our linear SVM.

```{r}
German_Credit.trctrl <- trainControl(method = "cv", number=10)
```


```{r}
set.seed(143)
svm_Linear <- train(RESPONSE ~., data = German_Credit.tr.subs, method = "svmLinear",
                    trControl=German_Credit.trctrl)
svm_Linear
```

We see that we have a good accuracy (0.72). 


```{r cache = TRUE}
grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))
grid

set.seed(143)
svm_Linear_Grid <- train(RESPONSE ~., data = German_Credit.tr.subs,
                         method = "svmLinear",
                         trControl=German_Credit.trctrl,
                         tuneGrid = grid)
svm_Linear_Grid

plot(svm_Linear_Grid)

svm_Linear_Grid$bestTune
```


### Tunning the hyperparameters of Radial basis SVM

```{r}
grid_radial <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),
                           C = c(1, 10, 100, 500, 1000))
grid_radial
```

```{r cache = TRUE}
set.seed(143)

svm_Radial_Grid <- train(RESPONSE ~., data = German_Credit.tr.subs, 
                         method = "svmRadial",
                         trControl=German_Credit.trctrl,
                         tuneGrid = grid_radial)

svm_Radial_Grid
```


```{r}
plot(svm_Radial_Grid)

svm_Radial_Grid$bestTune
```


```{r}
German_credit.rb.tuned <- svm(RESPONSE ~ .,data = German_Credit.tr.subs,
                              kernel = "radial", 
                              gamma = svm_Radial_Grid$bestTune$sigma,
                              cost = svm_Radial_Grid$bestTune$C)

German_credit.rb.tuned.pred <- predict(German_credit.rb.tuned, 
                                       newdata = German_credit.te)

confusionMatrix(data=German_credit.rb.tuned.pred, 
                reference = German_credit.te$RESPONSE)
```

We have an accuracy of 0.696 and a balanced accuracy of 0.6975. 


### Neural network - Simple hyperparameter tuning

To select the good parameters, we build a search grid and fit the model with each possible value in the grid. This is brute force and time consuming. The best model is selected among all the possible choices.
```{r cache = TRUE, message = FALSE, warning = FALSE, results = 'hide'}
set.seed(1)
fitControl <- trainControl(method = "cv", 
                           number = 10)

nnetGrid <-  expand.grid(size = seq(from = 1, to = 6, by = 1),
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))

nnetFit <- train(RESPONSE ~ ., 
                 data = German_credit.tr,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)

```

```{r}
plot(nnetFit)
```
The best Neural Networks parameters would be to choose (might be 3? to discuss) 4 hidden layers, with a decay of 0.3. 

```{r}
set.seed(345)
nn4 <- nnet(RESPONSE ~ ., data=German_credit.tr, size=4, decay = 0.3)

nn4_pred <- predict(nn4, type="class")
tab4 <- table(Obs=German_credit.tr$RESPONSE, Pred=nn4_pred) # confusion matrix
tab4
(acc4 <- sum(diag(tab4))/sum(tab4)) # accuracy

```
The accuracy of the neural network is of 82.13% 



## Next Analysis 

- Balance the data using either method. 
- Then, using **caret** and either CV or Bootstrap, put several models in competition. 
- Select the best one according to the choice of score. 
- Finally, use the test set to see if the best model does not overfit the training set.

By doing this, we will have achieved a complete supervised learning task from A to Z.



### Cross-validation with caret
The 10-CV can be easily obtained from **caret**. 

First, set up the splitting data method using the **trainControl** function.

```{r}
German_Credit.trctrl <- trainControl(method = "cv", number=10)
```

```{r}
German.Credit.cv <- train(RESPONSE ~., data = German_credit.tr,
                method = "glmStepAIC", 
                family="binomial",
                trControl=German_Credit.trctrl, trace=0)
German.Credit.cv
```


```{r}
German_Credit.pred <- predict(German.Credit.cv, newdata = German_credit.te)

confusionMatrix(data=German_Credit.pred, reference = German_credit.te$RESPONSE)
```


## Bootstrap with 10 replicates
We now apply the bootstrap with 10 replicates. Like for CV, we use **caret**.

The approach is the same as before. 
We only need to change the method in the **trainControl** function. The corresponding method is “boot632”.

100 replicates is veryyyy long to run... can do that on less sample ?? I put 10, takes 3 minutes for me

```{r cache = TRUE}

trctrl <- trainControl(method = "boot632", number=10)
German_credit.boot <- train(RESPONSE ~., data = German_credit.tr,
                            method = "glmStepAIC", family="binomial",
                            trControl=trctrl, trace = 0)

German_credit.boot
```

