---
title: "Analysis"
author: "Elodie Kwan and Katia Voltz"
date: '2022-04-26'
output:
  pdf_document: default
  html_document: default
---

```{r echo=FALSE, message=FALSE}
source("../scripts/setup.R")
```

### Import libraries and data

```{r}
German_credit_cleaned <- read.csv("./../Data_DA/GermanCredit_cleaned.csv", header = TRUE)

German_credit_cleaned$DURATION <- as.numeric(German_credit_cleaned$DURATION)
German_credit_cleaned$AMOUNT <- as.numeric(German_credit_cleaned$AMOUNT)
German_credit_cleaned$INSTALL_RATE <- as.numeric(German_credit_cleaned$INSTALL_RATE)
German_credit_cleaned$AGE <- as.numeric(German_credit_cleaned$AGE)
German_credit_cleaned$NUM_CREDITS <- as.numeric(German_credit_cleaned$NUM_CREDITS)
German_credit_cleaned$NUM_DEPENDENTS <- as.numeric(German_credit_cleaned$NUM_DEPENDENTS)

for (i in 1:ncol(German_credit_cleaned)){
  if (class(German_credit_cleaned[,i])=="integer"){
    German_credit_cleaned[,i] <- factor(German_credit_cleaned[,i])
    }
}

str(German_credit_cleaned)
```

### Traning set and Test set 
In order to fit some model, we are going to split our dataset into 2 sets. The **training set** and the **test set**. We do not forget to take the first variable **OBS.** out as it represents the index number for each observation. 

```{r}
set.seed(1234)
# indexes of the training set observations 
index.tr <- sample(x = 1:nrow(German_credit_cleaned), 
                   size= 3/4*nrow(German_credit_cleaned), replace=FALSE)
# training set 
German_credit.tr <- German_credit_cleaned[index.tr,-1]
# test set 
German_credit.te <- German_credit_cleaned[-index.tr,-1]
```

### Balancing the dataset

In this part, we apply the balancing data technique in order to improve the predictions of “Good Credit” and "Bad Credit", since we have more observations on the "Bad Credit".

The table below reveals the unbalanced problem.

```{r}
## Statistics on the training set
table(German_credit.tr$RESPONSE)
```

Indeed, we observe that the "Good Credit" (1) response appears **527** times in the training set and "Bad Credit" (0) **223**, two times less.
Since there are many more "Good Credit" than "Bad Credit", any model favors the prediction of the "Good Credit". It results a good accuracy but the specificity is low, as well as the balanced accuracy.


#### Sub-sampling
Balancing using sub-sampling consists of taking all the cases in the smallest class (here “Bad Credit”) and extract at random the same amount of cases in the largest category (here “Good”).

```{r}
n.Bad_Credit <- min(table(German_credit.tr$RESPONSE)) ## 32

## the "0" cases
German_credit.tr.Bad_Credit <- filter(German_credit.tr, RESPONSE == 0) 

## The "1" cases
German_credit.tr.Good_Credit <- filter(German_credit.tr, RESPONSE == 1)

## sub-sample 32 instances from the "Good Credit" by drawing indexes
index.no <- sample(size=n.Bad_Credit, x=1:nrow(German_credit.tr.Good_Credit), replace=FALSE) 

## Bind all the "Bad" and the sub-sampled "Good"
German_Credit.tr.subs <- data.frame(rbind(German_credit.tr.Bad_Credit,
                                          German_credit.tr.Good_Credit[index.no,])) 

## The cases are balanced
table(German_Credit.tr.subs$RESPONSE) 
```

The dataset is now balanced.


### Fitting a model : Classification Tree (Decision Tree)

Let's try a classification tree 
```{r}
german.ct <- rpart(RESPONSE ~ ., method = "class", data = German_Credit.tr.subs )
summary(german.ct)
```

Then we plot the classification tree. 

```{r}
rpart.plot(german.ct)
```

We see that among the 31 explanatory variables, this model uses 6 of them: **CHK_ACCT**, **AMOUNT**, **HISTORY**, **DURATION**, **GUARANTOR** and **EMPLOYMENT**. 
```{r}
german.bigct.prediction <- predict(german.ct, 
                                   newdata=German_credit.te,
                                   type="class")

confusionMatrix(data=german.bigct.prediction , reference = German_credit.te$RESPONSE)

```


As the tree is quite big and we want to know if we can prune it. 

#### Pruning the tree 

```{r}
printcp(german.ct)

plotcp(german.ct)
```

From the list of **cp** (complexity parameter), we would choose the line that has the lowest cross valiadation error. This can be seen on the column **xerror**. So the best cp would be 0.022422 with one split.  

From the graph, we can identify that, according to the 1-SE rule, the tree with 2 and 3 are equivalent. The tree with 3 nodes should be preferred. It appears below the dotted-line. 

The value of cp can be chosen arbitrarily between 0.018 and 0.095. So we decided to go with the suggested cp of 0.022 from the summary. 

```{r}
german.ct.prune <- prune(german.ct, cp=0.022)
rpart.plot(german.ct.prune)
```

This pruned decision tree with a cp of 0.022 uses the variables **CHK_ACCT** and **HISTORY**. 

Using this pruned tree we made by hand, we can compute the prediction and build a confusion matrix.

```{r}
german.ct.prediction <- predict(german.ct.prune, newdata=German_credit.te, type="class")

# Confusion Matrix
confusionMatrix(data=german.ct.prediction, reference = German_credit.te$RESPONSE)
```

The accuracy of the classification tree model is not really good (0.564), however we see that the balanced accuracy (0.6345) is a little bit better but still not enough. 

We decide to also look at what would an automatically pruned using 1-SE rule would give use and compare the model. 
```{r}
set.seed(1234)
german.ct.autoprune <- autoprune(RESPONSE ~ ., method = "class",
                                 data = German_Credit.tr.subs)
rpart.plot(german.ct.autoprune )
```


Here, only the variable **CHK_ACCT** is used. 


```{r}
german.ct.autoprune.prediction <- predict(german.ct.autoprune,
                                          newdata=German_credit.te,
                                          type="class")

# Confusion Matrix
confusionMatrix(data=german.ct.autoprune.prediction,
                reference = German_credit.te$RESPONSE)
```

The accuracy of the classification tree model is not really good (0.584) althought it is a little bit better than the one we fitted by hand. We also see that the balanced accuracy (0.6418) is a little bit better than its accuracy and the balanced accuracy of the other model, but still not enough.


#### Variable importance of the classification tree 

```{r}
varImp(german.ct.prune)
vip(german.ct.prune)
```

From this plot, we see that the variables that influences the most are **CHK_ACCT**, **HISTORY**, **SAV_ACCT**, **DURATION**, **AMOUNT** and **REAL_ESTATE**. They are not exactly the same as the one we saw above.

The variable **CHK_ACCT** and **HISTORY** were noticed though. 


### Fitting another model : Logistic Regression 
```{r}
# Logistic regression to see the significant variables (not working)
logreg <- glm(RESPONSE~., data = German_Credit.tr.subs, family= binomial)
summary(logreg)
```

We see that a lot of variables are not statistically significant for the model so we can think of a model reduction.

We can predict using the logistic regression. 

```{r}
german.logrer.pred_prob <- predict(logreg, newdata = German_credit.te,
                                    type="response")

german.logrer.pred <- ifelse(german.logrer.pred_prob >= 0.5, 1, 0)

# Confusion Matrix
confusionMatrix(data=as.factor(german.logrer.pred),
                reference = German_credit.te$RESPONSE)
```

From the confusion matrix summary, this model is better than the classification tree as the accuracy is higher (0.704) and that the balanced accuracy seems to be almost good as well ( 0.6852)


#### Variable selection and interpretation with step method (AIC criteria)

The stepwise variable selection is performed. 

```{r}
mod.logreg.sel <- step(logreg) # store the final model into mod.logreg.sel
summary(mod.logreg.sel)
```


The variables that have been removed are : **FURNITURE**, **RADIO.TV**, **EDUCATION**, **RETRAINING**, **MALE_DIV**, **MALE_SINGLE**, **MALE_MAR_or_WID**, **CO.APPLICANT**, **REAL_ESTATE**, **OWN_RES**, **NUM_CREDITS**, **JOB** and **NUM_DEPENDENTS**

In the end, we get the most significant model: 

$$ RESPONSE = -0.6339113 + 0.5970566 * CHK_{ACCT1} + 1.1123874 * CHK_{ACCT2} +
2.4109175 * CHK_{ACCT3} - 0.6393459 * HISTORY1 + 0.3153810 * HISTORY2 \\
+ 0.0245812 * HISTORY3 + 1.0638624 * HISTORY4 - 0.7159178 * NEW_{CAR1} \\
+ 1.3489217 * USED_{CAR1}  -0.8489518 * RETRAINING1 - 0.0002631 * AMOUNT \\
+ 0.5675624 * SAV_{ACCT1} - 0.0693577 * SAV_{ACCT2} + 0.5603771 * SAV_{ACCT3} \\
+ 1.3592948 * * SAV_{ACCT4} + 0.5542570 * EMPLOYMENT1 + 1.2338686 * EMPLOYMENT2 \\
+ 1.7999683 * EMPLOYMENT3 + 1.5521376 * EMPLOYMENT4 - 0.3278020 * INSTALL_{RATE} \\
+ 1.6927223 * GUARANTOR1 - 1.1117822 * PRESENT_{RESIDENT2} \\
- 0.3408387 * PRESENT_{RESIDENT3} - 0.7613632 * PRESENT_{RESIDENT4} \\
- 1.0532655 * PROP_{UNKN_NONE1} + 0.0181856 * AGE - 0.6281982 * OTHER_{INSTALL1} \\
- 0.8736712 * RENT1 + 0.5251823 * TELEPHONE + 1.2896516 * FOREIGN $$

$$ p = (e^{RESPONSE})/(1 + e^{RESPONSE})$$

It means that : 

+ The predicted probability of being a good applicant for **CHCK_ACCT3** is higher than for **CHK_ACCT0** (and also higher than for **CHK_ACCT1** and **CHK_ACCT2**).
+ The predicted probability of being a good applicant for **HISTORY1** is lower than for **HISTORY0**.
+ The predicted probability of being a good applicant for **HISTORY4** is higher than for **HISTORY0** (and also higher than for **HISTORY2** and **HISTORY3**).
+ The predicted probability of being a good applicant for **NEW_CAR1** is lower than for **NEW_CAR0**.
+ The predicted probability of being a good applicant for **USED_CAR1** is higher than for **USED_CAR0**.
+ The predicted probability of being a good applicant for **RETRAINING1** is lower than for **RETRAINING0**.
+ **AMOUNT** is negatively associated with **RESPONSE**.
+ The predicted probability of being a good applicant for **SAV_ACCT4** is higher than for **SAV_ACCT0** (and also higher than for **SAV_ACCT1** and **SAV_ACCT3**).
+ The predicted probability of being a good applicant for **SAV_ACCT2** lower than for **SAV_ACCT0**.
+ The predicted probability of being a good applicant for **EMPLOYMENT3** is higher than for **Employment0** (and also higher than for **EMPLOYMENT1**, **EMPLOYMENT2** and **EMPLOYMENT4**).
+ **INSTALL_RATE** is negatively associated with **RESPONSE**.
+ The predicted probability of being a good applicant for **GUARANTOR1 ** is higher than for **GUARANTOR0**.
+ The predicted probability of being a good applicant for **PRESENT_RESIDENT2** is lower than for **PRESENT_RESIDENT0** (and also lower than **PRESENT_RESIDENT3** and **PRESENT_RESIDENT4**).
+ The predicted probability of being a good applicant for **PROP_UNKN_NONE1** is lower than for **PROP_UNKN_NONE0**.
+ **AGE** is positively associated with **RESPONSE**.
+ The predicted probability of being a good applicant for **OTHER_INSTALL1** is lower than for **OTHER_INSTALL0**.
+ The predicted probability of being a good applicant for **RENT1** is lower than for **RENT0**.
+ The predicted probability of being a good applicant for **TELEPHONE1** is higher than for **TELEPHONE0**.
+ The predicted probability of being a good applicant for **FOREIGN1** is higher than for **FOREIGN0**.


```{r}
german.logregsel.pred_prob <- predict(mod.logreg.sel, newdata = German_credit.te,
                                    type="response")

german.logregsel.pred <- ifelse(german.logregsel.pred_prob  >= 0.5, 1, 0)

# Confusion Matrix
confusionMatrix(data=as.factor(german.logregsel.pred),
                reference = German_credit.te$RESPONSE)
```


#### Variable importance for logistic regression

```{r}
x_train <- select(German_Credit.tr.subs, -RESPONSE)
y_train <- pull(German_Credit.tr.subs, RESPONSE)

explainer_logreg <- explain(model = mod.logreg.sel, 
                               data = x_train, 
                               y = as.numeric(y_train), 
                               label = "Logistic Regression")


importance_logreg  <- calculate_importance(explainer_logreg)
plot(importance_logreg)
```

```{r}
vip(mod.logreg.sel)
```

Listed above are the most important variables for the logarithmic regression we reduced. 

### Fitting another model : KNN
To perform a k-nearest neighbor method, we do not need to balance the data so we will use the unbalanced training set.
Now, we make the prediction using a 2-NN (with Euclidean distance).

```{r}
German_credit.num.response <- German_credit_cleaned %>% 
  select('RESPONSE','DURATION', 'AMOUNT', 
         'INSTALL_RATE', 'AGE', 'NUM_CREDITS','NUM_DEPENDENTS')
```

```{r}
set.seed(123)
## build the K-NN model with k = 2 
German_credit.knn <- knn3(data=German_credit.num.response, RESPONSE~., k=2) 

German_credit.knn.pred <- predict(German_credit.knn, 
                                  newdata = German_credit.num.response,
                                  type="class")
```

Now we can use the 2-NN to predict the test set using the training set. 
Note that the model is fitting on the training set and the predictions are computed on the test set.

```{r}
set.seed(123)
# applying Knn model with k = 2 on the training set
German_credit.knn.tr <- knn3(data=German_credit.tr, RESPONSE~., k=2)  

German_credit.knn.tr.pred <- predict(German_credit.knn.tr, 
                                     newdata = German_credit.te, type="class")
```

To compare the predictions above and the true state of the applicant (the one in the test set), we can build a table. It is called a **confusion matrix** (again, this will be detailed later on).

The table is read as follow : 

+ We predicted 21 Bad credits and there were indeed 21 observed Bad credits. But the prediction misjudges 45 good credits by predicting bad credits.
+ We predicted 128 Good credits as it was in fact a Good credits but 56 where predicted as Good while it was in fact Bad. 

```{r}
# Confusion Matrix
confusionMatrix(data=German_credit.knn.tr.pred,
                reference = German_credit.te$RESPONSE)
```

The accuracy (0.596) and the unbalanced accuracy (0.5063) are both too low. 

The prediction is not perfect. We need to try to improve the prediction by changing K at that point. Therefore, we use K=3.

```{r}
set.seed(123)
# applying Knn model with k = 3 on the training set
German_credit.knn3.tr <- knn3(data=German_credit.tr, RESPONSE~., k=3)  

German_credit.knn3.tr.pred <- predict(German_credit.knn3.tr, 
                                      newdata = German_credit.te, type="class")
```

The table is read as follow : 

+ We predicted 14 Bad credits and they were indeed observed Bad credits. But the prediction misjudges 28 good credits by predicting bad credits.
+ We predicted 145 Good credits as it was in fact a Good credits but 6 where predicted as Good while it was in fact Bad. 


```{r}
# Confusion Matrix
confusionMatrix(data=German_credit.knn3.tr.pred,
                reference = German_credit.te$RESPONSE)
```

Both the accuracy (0.636) and the balanced data (0.5100) improved a little bit with k = 3 compared to k = 2. 
The accuracies might still be a bit low.


### Linear Support Vector Machine


...

```{r}
German_credit.svm <- svm(RESPONSE ~ ., data=German_Credit.tr.subs, kernel="linear")
German_credit.svm
German_credit.svm.pred <- predict(German_credit.svm, newdata = German_credit.te)
```

```{r}
confusionMatrix(data=German_credit.svm.pred, reference = German_credit.te$RESPONSE )
```

The accuracy (0.692) and the balanced accuracy (0.6802) are lower than 0.75 which means that it might not be good enough. 


### Radial basis Support Vector Machine
We try now with a radial basis kernel (the default).

```{r}
German_credit.rb <- svm(RESPONSE ~ ., data=German_Credit.tr.subs, kernel="radial")
German_credit.rb

German_credit.pred <- predict(German_credit.rb, newdata = German_credit.te)
confusionMatrix(data=German_credit.pred, reference = German_credit.te$RESPONSE )
```
The accuracy is better, 70%, compared to 69% with the linear method.



### Tunning the hyperparameters of Linear SVM

We want to select the good hyperparameters for our linear SVM.

```{r}
German_Credit.trctrl <- trainControl(method = "cv", number=10)
```


```{r}
set.seed(143)
svm_Linear <- train(RESPONSE ~., data = German_Credit.tr.subs, method = "svmLinear",
                    trControl=German_Credit.trctrl)
svm_Linear
```

We see that we have a good accuracy (0.72). 


```{r cache = TRUE}
grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))
grid

set.seed(143)
svm_Linear_Grid <- train(RESPONSE ~., data = German_Credit.tr.subs,
                         method = "svmLinear",
                         trControl=German_Credit.trctrl,
                         tuneGrid = grid)
svm_Linear_Grid

plot(svm_Linear_Grid)

svm_Linear_Grid$bestTune
```

```{r}
German_credit.lsvm.tuned <- svm(RESPONSE ~ .,data = German_Credit.tr.subs,
                              kernel = "linear", 
                              cost = svm_Linear_Grid$bestTune$C)

German_credit.lsvm.tuned.pred <- predict(German_credit.lsvm.tuned, 
                                       newdata = German_credit.te)

confusionMatrix(data=German_credit.lsvm.tuned.pred, 
                reference = German_credit.te$RESPONSE)
```

### Tunning the hyperparameters of Radial basis SVM

```{r}
grid_radial <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),
                           C = c(1, 10, 100, 500, 1000))
grid_radial
```

```{r cache = TRUE}
set.seed(143)

svm_Radial_Grid <- train(RESPONSE ~., data = German_Credit.tr.subs, 
                         method = "svmRadial",
                         trControl=German_Credit.trctrl,
                         tuneGrid = grid_radial)

svm_Radial_Grid
```


```{r}
plot(svm_Radial_Grid)

svm_Radial_Grid$bestTune
```


```{r}
German_credit.rb.tuned <- svm(RESPONSE ~ .,data = German_Credit.tr.subs,
                              kernel = "radial", 
                              gamma = svm_Radial_Grid$bestTune$sigma,
                              cost = svm_Radial_Grid$bestTune$C)

German_credit.rb.tuned.pred <- predict(German_credit.rb.tuned, 
                                       newdata = German_credit.te)

confusionMatrix(data=German_credit.rb.tuned.pred, 
                reference = German_credit.te$RESPONSE)
```

We have an accuracy of 0.696 and a balanced accuracy of 0.6975. 



### Neural network - Simple hyperparameter tuning

To select the good parameters, we build a search grid and fit the model with each possible value in the grid. This is brute force and time consuming. The best model is selected among all the possible choices.
```{r cache = TRUE, message = FALSE, warning = FALSE, results = 'hide'}
set.seed(1)
fitControl <- trainControl(method = "cv", 
                           number = 10)

nnetGrid <-  expand.grid(size = seq(from = 1, to = 6, by = 1),
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))

nnetFit <- train(RESPONSE ~ ., 
                 data = German_Credit.tr.subs,
                 method = "nnet",
                 metric = "Accuracy",
                 tuneGrid = nnetGrid,
                 trControl = fitControl)

```

```{r}
plot(nnetFit)
```
The best Neural Networks parameters would be to choose 3 hidden layers, with a decay of 0.4. 

```{r}
set.seed(345)
nn4 <- nnet(RESPONSE ~ ., data=German_Credit.tr.subs, size=3, decay = 0.4)

nn4_pred <- predict(nn4, newdata = German_credit.te, type="class")

confusionMatrix(data= as.factor(nn4_pred), 
                reference = German_credit.te$RESPONSE)

```
The accuracy of the neural network is of 81.61% 


### Gradient Boosting  
The Gradient Boosting model accepts only numerical values so we have some transformation to do on our data in order to use it. 

```{r}
Gradient_boost.y_train <- as.integer(German_Credit.tr.subs$RESPONSE)-1
Gradient_boost.y_test <- as.integer(German_credit.te$RESPONSE)-1

Gradient_boost.X_train <- sparse.model.matrix(RESPONSE ~ .-1, 
                                              data = German_Credit.tr.subs )
Gradient_boost.X_test <- sparse.model.matrix(RESPONSE ~ .-1,
                                             data = German_credit.te )

```


```{r}
set.seed(123)
xgb_train <- xgb.DMatrix(data = Gradient_boost.X_train,
                         label = Gradient_boost.y_train)

xgb_test <- xgb.DMatrix(data = Gradient_boost.X_test, 
                        label = Gradient_boost.y_test)

xgb_params <- list(
  booster = "gbtree",
  eta = 0.01,
  max_depth = 8,
  gamma = 4,
  subsample = 0.75,
  colsample_bytree = 1,
  objective = "multi:softmax",
  eval_metric = "mlogloss",
  num_class = 2
  )

xgb_model <- xgb.train(
  params = xgb_params,
  data = xgb_train,
  nrounds = 5000,
  verbose = 1
  )

xgb_model
```


```{r}
xgb_preds <- predict(xgb_model, Gradient_boost.X_test, reshape = TRUE)
```

Here we have an accuracy of 68.4%. It is good but there is room for improvement. 

```{r}
confusionMatrix(data=as.factor(xgb_preds), 
                reference = as.factor(Gradient_boost.y_test))
```




## Next Analysis 

- Balance the data using either method. 
- Then, using **caret** and either CV or Bootstrap, put several models in competition. 
- Select the best one according to the choice of score. 
- Finally, use the test set to see if the best model does not overfit the training set.

By doing this, we will have achieved a complete supervised learning task from A to Z.



### Cross-validation with caret
The 10-CV can be easily obtained from **caret**. 

First, set up the splitting data method using the **trainControl** function.

```{r}
set.seed(123)
German_Credit.trctrl <- trainControl(method = "cv", number=10)
German.Credit.cv <- train(RESPONSE ~., data = German_credit.tr,
                method = "glmStepAIC", 
                family="binomial",
                trControl=German_Credit.trctrl, trace=0)
German.Credit.cv
```


```{r}
German_Credit.cv.pred <- predict(German.Credit.cv, newdata = German_credit.te)

confusionMatrix(data=German_Credit.cv.pred, reference = German_credit.te$RESPONSE)
```


## Bootstrap with 10 replicates
We now apply the bootstrap with 10 replicates. Like for CV, we use **caret**.

The approach is the same as before. 
We only need to change the method in the **trainControl** function. The corresponding method is “boot632”.

100 replicates is veryyyy long to run... can do that on less sample ?? I put 10, takes 3 minutes for me

```{r cache = TRUE}
set.seed(123)
trctrl <- trainControl(method = "boot632", number=10)
German_credit.boot <- train(RESPONSE ~., data = German_credit.tr,
                            method = "glmStepAIC", family="binomial",
                            trControl=trctrl, trace = 0)

German_credit.boot
```


```{r}
German_Credit.boot.pred <- predict(German_credit.boot, newdata = German_credit.te)

confusionMatrix(data=German_Credit.boot.pred, reference = German_credit.te$RESPONSE)
```




